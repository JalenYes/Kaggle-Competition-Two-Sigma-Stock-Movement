{"cells":[{"metadata":{"_uuid":"225708f447eee93041881f9d6c3a3e890cb16718"},"cell_type":"markdown","source":"## A simple LSTM model\n\nI combined techinques and code from two notebooks that I found, and converted it to an LSTM. \n\nReferences to other notebooks used: \n\nhttps://www.kaggle.com/christofhenkel/market-data-nn-baseline\n\nhttps://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c20fa6deeac9d374c98774abd90bdc76b023ee63"},"cell_type":"code","source":"# get the data from two sigma environment\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8174e0fe351b1c2c667fb463406975e941aba9c"},"cell_type":"code","source":"# gets used later to aggregate news into marker data \nnews_cols_agg = {\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n}\n# specify categorical columns\ncategorical_cols = ['assetName', 'dayofweek', 'month', 'year']\n# lengths of embeddings of categorical columns\nembedding_lengths = [100, 2, 2, 3]\nencodings = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c5a87dde9b7931382da32d0b362116fb8fbc94c"},"cell_type":"code","source":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes -- converts ['AAPL', 'GOOG'] --> 'APPL', 'GOOG'\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"969511254abbe99bfbb38c4e85422596635cbd30"},"cell_type":"code","source":"def get_xy(market_train_df, news_train_df):\n    x = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y\n\ndef label_encode(series, min_counts=2):\n    vc = series.value_counts()\n    #reserve 0 for unknown\n    le = {c : i+1 for i, c in enumerate(vc.index[vc > min_counts])}\n    le['UNKN'] = 0\n    return le\n\ndef get_encodings(df, cat_cols):\n    if len(encodings) == 0:\n        for col in cat_cols:\n            encodings[col] = label_encode(df[col])\n    return encodings\n\ndef map_encodings(df, cat_cols, encs):\n    for col in cat_cols:\n        df[col] = df[col].map(encs[col]).fillna(0).astype(int)\n        \ndef get_x(market_train_df, news_train_df, isTrain=True):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)    \n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    x['dayofweek'], x['day'], x['month'], x['year'] = x.time.dt.dayofweek, x.time.dt.day, x.time.dt.month, x.time.dt.year\n\n    encodings = get_encodings(x, categorical_cols)\n    map_encodings(x, categorical_cols, encodings) \n    if isTrain:\n        cols_to_drop = ['returnsOpenNextMktres10', 'universe', 'time']\n    else: \n        cols_to_drop = ['time']\n    \n    x.drop(columns=cols_to_drop, inplace=True)\n        \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31cd42303a383d2a6537c6f7b99ec2d298f26ff7"},"cell_type":"code","source":"# This will take some time...\nX, y = get_xy(market_train_df, news_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"068cd43c59e905593126201737f8a82d1024cdfb"},"cell_type":"code","source":"#Save universe data for latter use\nuniverse = market_train_df['universe']\ntime = market_train_df['time']\n\n# Free memory\ndel market_train_df, news_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f3fdd10e5debe4d487c0b52b944df75bf13e62f"},"cell_type":"code","source":"#get all the numeric columns\nnum_cols = [x for x in X.columns if x not in categorical_cols]\n\n#remove assetCode from num_cols\nnum_cols = [x for x in num_cols if x not in ['assetCode']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94cec422fc877d8aba81c797d34b06d2fdff21b1","scrolled":true},"cell_type":"code","source":"#scale numeric cols\ndef scale_numeric(df):\n    df[num_cols] = df[num_cols].fillna(0)\n\n    scaler = StandardScaler()\n    \n    #need to do this due to memory contraints\n    for i in range(0, len(num_cols), 4):\n        cols = num_cols[i:i + 3]\n        df[cols] = scaler.fit_transform(df[cols].astype(float))\n        \nscale_numeric(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"676b28bbf482d680e642c6c7450bf5c918399467"},"cell_type":"code","source":"#split dataset into 80% for training and 20% for validation \nn_train = int(X.shape[0] * 0.8)\n\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a172258a46c7db96e85964d6d56c24f0ec56f0c"},"cell_type":"code","source":"# For valid data, keep only those with universe > 0. This will help calculate the metric\nu_valid = (universe.iloc[n_train:] > 0)\nt_valid = time.iloc[n_train:]\n\nX_valid = X_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]\n\nd_valid = t_valid.dt.date\n\ndel u_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a8d1bb2d2d2a4b4a9202912578a0255bace89c2"},"cell_type":"code","source":"#seperate the columns into categorical and numerical\ndef get_cat_num_split(df):\n    X = {} \n    X['num'] = df.loc[:, num_cols].values\n    X['num'] = np.reshape(X['num'], (X['num'].shape[0], 1, X['num'].shape[1]))\n    for cat in categorical_cols:\n        X[cat] = df.loc[:, cat].values\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ba3082b4c57fdc6016571cb4688f262512aa6ff"},"cell_type":"code","source":"#seperate the columns into categorical and numerical\nX_train_split = get_cat_num_split(X_train)\nX_valid_split = get_cat_num_split(X_valid)\n\n#set y to a binary representation of returns, true if it's 0-1 and false if i'ts -1-0\ny_train_bin = (y_train >= 0).values\ny_valid_bin = (y_valid >= 0).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93f4e47393778cbe5040e3aee5fc6fdc5505c41e"},"cell_type":"code","source":"encoding_len = {k: len(encodings[k]) + 1 for k in encodings}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df9c3f37a0344522e9f8ed3253242d3c00ac9114"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, LSTM, Dropout, Reshape\nfrom keras.losses import binary_crossentropy, mse\nfrom keras.regularizers import l2\nimport keras.backend as K\n\nDROPOUT_RATE = 0.2\n\ncat_inputs = [Input(shape=[1], name=cat) for cat in categorical_cols]\nembeddings = [Embedding(encoding_len[cat], embedding_lengths[i])(cat_inputs[i]) for i, cat in enumerate(categorical_cols)]\ncategorical_logits = Concatenate()([(cat_emb) for cat_emb in embeddings])\ncategorical_logits = LSTM(128, activation='relu', input_shape=(1, len(categorical_cols)), return_sequences=True,\n                         kernel_regularizer=l2(1e-5), kernel_initializer='random_uniform')(categorical_logits)\n\nnumerical_inputs = Input(shape=(1, len(num_cols)), name='num')\nnumerical_logits = LSTM(256, activation='relu', input_shape=(1, len(num_cols)), return_sequences=True,\n                        kernel_regularizer=l2(1e-5), kernel_initializer='random_uniform')(numerical_inputs)\nnumerical_logits = Dropout(DROPOUT_RATE)(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = LSTM(256, activation='relu', kernel_initializer='random_uniform')(logits)\nout = Dense(1, activation='sigmoid', name='confidence_level')(logits)\n\nmodel = Model(inputs = cat_inputs + [numerical_inputs], outputs=out)\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ea6bee7b105d40125e546097fb5cb9eabe56e87"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d221c6488b13ef0b5be8c54fc1ccc6b11bbe858"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=1,verbose=True)\nmodel.fit(X_train_split, y_train_bin,\n          validation_data=(X_valid_split, y_valid_bin),\n          epochs=1,\n          verbose=True,\n          callbacks=[early_stop,check_point]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73a12439b5d43ee8d054d8b7b32c3de09b757543"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# distribution of confidence that will be used as submission\nmodel.load_weights('model.hdf5')\n# scaled condifence value  from 0 - 1 to -1 - 1\nconfidence_valid = model.predict(X_valid_split)[:,0]*2 -1\n\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d49cc34a1c7e7a292744c93e4b6891290052561d"},"cell_type":"code","source":"#r_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * y_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79f77f74583568d57c5caec118ffa8e00687b1e3"},"cell_type":"markdown","source":"# Train full model\nNow we train a full model with `num_boost_round` found in validation."},{"metadata":{"trusted":true,"_uuid":"3f8925ddb4c42201fdba38efac258ca38773849d","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"def make_predictions(predictions_template_df, market_obs_df, news_obs_df):\n    inp = get_x(market_obs_df, news_obs_df, False)\n    scale_numeric(inp)\n    inp_split = get_cat_num_split(inp)\n    scaled_pred = model.predict(inp_split) * 2 - 1    \n    predictions_template_df.confidenceValue = np.clip(scaled_pred, -1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"724c38149860c8e9058474ac9045c2301e8a20da","scrolled":false},"cell_type":"code","source":"days = env.get_prediction_days()\n\nx1,y1,z1 = None, None, None\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    x1,y1,z1 = predictions_template_df, market_obs_df, news_obs_df\n    make_predictions(x1,y1,z1)\n    env.predict(predictions_template_df)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}